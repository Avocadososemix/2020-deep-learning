{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Thursday, May 20, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory. \n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "from os.path import abspath\n",
    "from os.path import split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "import zipfile\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    \n",
    "train_path = 'train'\n",
    "\n",
    "data_folder_name = 'text-training-corpus'\n",
    "DATA_FOLDER_DIR = os.path.abspath(os.path.join(root_dir, data_folder_name))\n",
    "\n",
    "data_zip_name = 'reuters-training-corpus.zip'\n",
    "DATA_ZIP_DIR = os.path.abspath(os.path.join(DATA_FOLDER_DIR, data_zip_name))\n",
    "\n",
    "with zipfile.ZipFile(DATA_ZIP_DIR) as zip_f:\n",
    "    zip_f.extractall(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.  \n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lxml\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.abspath(os.path.join(root_dir, train_path))\n",
    "reuters_unzipped_path = os.path.abspath(os.path.join(train_path, 'REUTERS_CORPUS_2'))\n",
    "\n",
    "zipped_news_files = os.listdir(os.path.abspath(reuters_unzipped_path))\n",
    "del zipped_news_files[-3:] # remove files: codes.zip, dtds.zip, readme.txt\n",
    "#zipped_news_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating dataframe. Takes ~5min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "pattern = r'\"([A-Za-z0-9_\\./\\\\-]*)\"'\n",
    "\n",
    "for news_file in zipped_news_files:\n",
    "    zf = zipfile.ZipFile(os.path.abspath(os.path.join(reuters_unzipped_path, news_file)), 'r')\n",
    "    for name in zf.namelist():\n",
    "        dict1 = {} # saving into dicts which are then saved into a list which is then saved into the df.\n",
    "        f = zf.open(name).read()\n",
    "        soup = BeautifulSoup(f, \"xml\")\n",
    "\n",
    "        title = soup.title.text\n",
    "        dict1[\"title\"] = soup.title.text\n",
    "        text = soup.find(\"text\").text\n",
    "        dict1[\"text\"] = soup.find(\"text\").text\n",
    "        codes = []\n",
    "        metadata_codes = soup.metadata.find_all(\"code\")\n",
    "        for val in metadata_codes:\n",
    "            m = re.search(pattern, str(val))\n",
    "            codes.append(m.group().replace('\"', ''))\n",
    "        dict1[\"codes\"] = codes\n",
    "        \n",
    "        rows_list.append(dict1)\n",
    "\n",
    "df = pd.DataFrame(rows_list, columns=['title', 'text', 'codes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>EU:  REUTER EC REPORT LONG-TERM DIARY FOR APR ...</td>\n",
       "      <td>\\n****\\nHIGHLIGHTS\\n****\\nAMSTERDAM - The Neth...</td>\n",
       "      <td>[EEC, G15, GCAT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>EU:  OFFICIAL JOURNAL CONTENTS - OJ L 85 OF MA...</td>\n",
       "      <td>\\n* Decision of the EEA Joint Committee No 55/...</td>\n",
       "      <td>[EEC, G15, GCAT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CANADA: Toronto stocks end higher after volati...</td>\n",
       "      <td>\\nCHANGE\\t\\t\\t\\t    CHANGE\\nTSE\\t  5900.37    ...</td>\n",
       "      <td>[CANA, M11, MCAT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CANADA: TSE says will not halt Bre-X on request.</td>\n",
       "      <td>\\nAfter a huge volume of trade in Bre-X Minera...</td>\n",
       "      <td>[CANA, I21000, C13, C14, C15, C152, CCAT, M11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CANADA: Suncor lowers Canada posted oil prices.</td>\n",
       "      <td>\\nSuncor Inc said it lowered the price it woul...</td>\n",
       "      <td>[CANA, M14, M143, MCAT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299768</td>\n",
       "      <td>USA: UPS says has deal to end Teamsters' strike.</td>\n",
       "      <td>\\nUnited Parcel Service said on Monday it had ...</td>\n",
       "      <td>[USA, GJOB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299769</td>\n",
       "      <td>USA: UPS says has tentative deal to end strike.</td>\n",
       "      <td>\\nUnited Parcel Service said late Monday night...</td>\n",
       "      <td>[USA, I79010, C42, CCAT, E41, ECAT, GCAT, GJOB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299770</td>\n",
       "      <td>JAPAN: Asia currency woes hurt region's oil pr...</td>\n",
       "      <td>\\nThis year's rash of Asian currency crises co...</td>\n",
       "      <td>[INDON, JAP, MALAY, PHLNS, THAIL, C21, C24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299771</td>\n",
       "      <td>TAIWAN: Typhoon Winnie kills 25 in Taiwan.</td>\n",
       "      <td>\\nA typhoon that packed high winds and torrent...</td>\n",
       "      <td>[TAIWAN, GDIS, GENV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299772</td>\n",
       "      <td>USA: Maverick Records announces management shift.</td>\n",
       "      <td>\\nFreddy DeMann said Monday he was relinquishi...</td>\n",
       "      <td>[USA]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299773 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0       EU:  REUTER EC REPORT LONG-TERM DIARY FOR APR ...   \n",
       "1       EU:  OFFICIAL JOURNAL CONTENTS - OJ L 85 OF MA...   \n",
       "2       CANADA: Toronto stocks end higher after volati...   \n",
       "3        CANADA: TSE says will not halt Bre-X on request.   \n",
       "4         CANADA: Suncor lowers Canada posted oil prices.   \n",
       "...                                                   ...   \n",
       "299768   USA: UPS says has deal to end Teamsters' strike.   \n",
       "299769    USA: UPS says has tentative deal to end strike.   \n",
       "299770  JAPAN: Asia currency woes hurt region's oil pr...   \n",
       "299771         TAIWAN: Typhoon Winnie kills 25 in Taiwan.   \n",
       "299772  USA: Maverick Records announces management shift.   \n",
       "\n",
       "                                                     text  \\\n",
       "0       \\n****\\nHIGHLIGHTS\\n****\\nAMSTERDAM - The Neth...   \n",
       "1       \\n* Decision of the EEA Joint Committee No 55/...   \n",
       "2       \\nCHANGE\\t\\t\\t\\t    CHANGE\\nTSE\\t  5900.37    ...   \n",
       "3       \\nAfter a huge volume of trade in Bre-X Minera...   \n",
       "4       \\nSuncor Inc said it lowered the price it woul...   \n",
       "...                                                   ...   \n",
       "299768  \\nUnited Parcel Service said on Monday it had ...   \n",
       "299769  \\nUnited Parcel Service said late Monday night...   \n",
       "299770  \\nThis year's rash of Asian currency crises co...   \n",
       "299771  \\nA typhoon that packed high winds and torrent...   \n",
       "299772  \\nFreddy DeMann said Monday he was relinquishi...   \n",
       "\n",
       "                                                    codes  \n",
       "0                                        [EEC, G15, GCAT]  \n",
       "1                                        [EEC, G15, GCAT]  \n",
       "2                                       [CANA, M11, MCAT]  \n",
       "3       [CANA, I21000, C13, C14, C15, C152, CCAT, M11,...  \n",
       "4                                 [CANA, M14, M143, MCAT]  \n",
       "...                                                   ...  \n",
       "299768                                        [USA, GJOB]  \n",
       "299769    [USA, I79010, C42, CCAT, E41, ECAT, GCAT, GJOB]  \n",
       "299770        [INDON, JAP, MALAY, PHLNS, THAIL, C21, C24]  \n",
       "299771                               [TAIWAN, GDIS, GENV]  \n",
       "299772                                              [USA]  \n",
       "\n",
       "[299773 rows x 3 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set.\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
